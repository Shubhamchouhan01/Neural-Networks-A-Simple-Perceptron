{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15c73c9",
   "metadata": {},
   "source": [
    "# Neural Network â€“ A Simple Perceptron (PW Skills Assignment)\n",
    "\n",
    "**Subject:** Neural Networks\n",
    "\n",
    "This assignment is written in a proper, student-style format. All questions are numbered, explanations are simple and human-written, and code is placed only in code cells as instructed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b412f0",
   "metadata": {},
   "source": [
    "## 1. What is Deep Learning, and how is it connected to Artificial Intelligence?\n",
    "\n",
    " ans --Deep Learning (DL) is a parametric framework used to predict outcomes by creating a function of parameters. It is characterized by the following key features:\n",
    "\n",
    "Parametric Form: It follows the equation \n",
    "y=f(x)\n",
    ".Performance and Scale: Unlike traditional Machine Learning (ML), the performance of Deep Learning continues to improve as the scale of data increases.\n",
    "Automated Feature Engineering: Deep Learning is capable of automated feature engineering, which allows it to process complex data types like images, videos, text, audio, and graphs.\n",
    "Neural Network Architectures: It utilizes various networks depending on the data type, such as:\n",
    "Artificial Neural Networks (ANN): For tabular data.\n",
    "\n",
    "Convolutional Neural Networks (CNN): For image data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3befd1b",
   "metadata": {},
   "source": [
    "## 2. What is a neural network, and what are its types?\n",
    "\n",
    " ans-- A Neural Network is a framework used to predict an output ($y$) by creating a function of parameters. It is designed to mimic the way a brain works using the following components:Neuron: The basic building block that involves inputs ($x$), weights ($w$), and biases ($b$).\n",
    " Mathematical Foundation: It operates on the parametric form $y \\approx f(x)$, utilizing slopes, intercepts, and curvatures to define the model.+2Architecture: It consists of an input layer, Hidden Layers (HL), and an output layer ($y$).\n",
    "\n",
    "Types of Neural Networks\n",
    "Neural networks are categorized based on the specific type of data they are designed to process:\n",
    "Artificial Neural Networks (ANN)\n",
    "Convolutional Neural Networks (CNN)\n",
    "Recurrent Neural Networks (RNN/LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952674d0",
   "metadata": {},
   "source": [
    "## 3. What is the mathematical structure of a neural network?\n",
    "ans-- The structure is defined by the following mathematical components:\n",
    "1. The Core EquationThe network follows the fundamental parametric equation:$$y = f(x)$$.\n",
    "The goal is to make the predicted value of $y$ as close as possible to the actual value by modifying parameters.\n",
    "2. Components of a NeuronA single neuron processes data using the following variables:Inputs ($x$): The data features (e.g., $x_1, x_2, x_3$).+3Weights ($w$): Values like $w_1, w_2, w_3$ that determine the strength of the input signal.+2Bias/Intercept ($b$): An additional parameter (e.g., $b_1, b_2$) added to the weighted sum of inputs.+3Slope and Curvature: Concepts used to define the mathematical relationship within the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414c7f6",
   "metadata": {},
   "source": [
    "## 4. What is an activation function and why is it essential?\n",
    "\n",
    "ans-- What is an Activation Function?An activation function is a mathematical component applied to the weighted sum of inputs and biases within a neuron. In the lecture notes, it is represented as a function that transforms the linear combination of inputs into a specific output format.\n",
    "Mathematical Representation: It is often shown using functions like Sigmoid, \n",
    "which follows the structure: $y = \\text{Sigmoid}(w_1x_1 + w_2x_2 + b_1)$.\n",
    "Role in the Neuron: After the neuron calculates the weighted sum (e.g., $w_1x_1 + w_2x_2$), the activation function determines the final signal that is passed forward to the next layer.\n",
    "Why is it Essential?\n",
    "The activation function is a critical part of the neural network architecture for several reasons:Defining Relationships: It helps the model handle mathematical properties like slopes, intercepts, and curvatures, which are necessary to map complex data.+1Non-Linearity: While traditional linear regression uses simple equations, activation functions allow the network to function as a non-parametric process or a complex parametric form capable of solving non-linear problems.\n",
    "utcome Prediction: It is essential for the process where we want the predicted output ($y$) to be as close as possible to the actual value.Predictive Framework: It allows Deep Learning to become a framework that can predict outcomes based on varied data types like images, audio, and text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c8a1e",
   "metadata": {},
   "source": [
    "## 5. List common activation functions.\n",
    "\n",
    "An activation function is a mathematical component applied to the weighted sum of inputs and biases within a neuron. In the lecture notes, it is represented as a function that transforms the linear combination of inputs into a specific output format\n",
    "Mathematical Representation: It is often shown using functions like Sigmoid, which follows the structure: $y = \\text{Sigmoid}(w_1x_1 + w_2x_2 + b_1)$.\n",
    "Role in the Neuron: After the neuron calculates the weighted sum (e.g., $w_1x_1 + w_2x_2$), the activation function determines the final signal that is passed forward to the next layer.\n",
    "\n",
    "Why is it Essential?\n",
    "The activation function is a critical part of the neural network architecture for several reasons:\n",
    "\n",
    "\n",
    "Defining Relationships: It helps the model handle mathematical properties like slopes, intercepts, and curvatures, which are necessary to map complex data\n",
    "\n",
    "Non-Linearity: While traditional linear regression uses simple equations, activation functions allow the network to function as a non-parametric process or a complex parametric form capable of solving non-linear problems\n",
    "Outcome Prediction: It is essential for the process where we want the predicted output ($y$) to be as close as possible to the actual value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279819b7",
   "metadata": {},
   "source": [
    "\n",
    "## 6. What is a multilayer neural network?\n",
    "Layered Architecture: It consists of an Input Layer, Hidden Layers (HL), and an Output Layer ($y$).\n",
    "Hidden Layers (HL): These are the intermediate layers between input and output where complex mathematical transformations occur.\n",
    "Mathematical Processing: Within these layers, neurons calculate weighted sums of inputs (e.g., $w_1x_1 + w_2x_2$) and add biases ($b$).\n",
    "Activation Functions: Each layer typically applies an activation function, such as Sigmoid, to the results of its calculations before passing them to the next layer.\n",
    "\n",
    "Deep Learning Connection: When a network has many hidden layers, it is referred to as Deep Learning, which allows for automated feature engineering on complex data like images and video\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d53255",
   "metadata": {},
   "source": [
    "## 7. What is a loss function and why is it important?\n",
    "A loss function (referred to in the notes as \"Loss\") is a measure of the difference between the predicted output ($y$) and the actual ground truth. The primary goal of a neural network is to make the predicted value of $y$ as close as possible to the actual target by minimizing this loss\n",
    "Why is it Important?\n",
    "The loss function is essential for the following reasons:\n",
    "Guided Learning: It provides a numerical value that represents the error of the model.\n",
    "Optimization via Gradient Descent: It serves as the basis for Gradient Descent, which is the process used to reduce the error.\n",
    "Parameter Tuning: It informs the system exactly how much to modify the weights ($w$) and biases ($b$) to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf820a0",
   "metadata": {},
   "source": [
    "## 8. Common types of loss functions.\n",
    "Mean Squared Error (MSE): While the notes do not explicitly name it \"MSE,\" they categorize Regression as a primary task where loss is calculated to predict continuous values like Price.\n",
    "Binary Cross-Entropy / Log Loss: This is used for Classification tasks. The notes explicitly mention Logistic Regression (Log Rig) and the use of the Sigmoid activation function , which are mathematically tied to calculating loss for binary outcomes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d7b29",
   "metadata": {},
   "source": [
    "## 9. How does a neural network learn?\n",
    "Architecture Definition: The process begins by defining the structure of the network, including the input, hidden (HL), and output layers.\n",
    "Initialization: The weights ($w$) and biases ($b$) are initially assigned random values.Prediction (Forward Pass): The network takes inputs ($x$) and processes them through neurons using mathematical functions, such as $y \\approx f(x_1, x_2)$, and activation functions like Sigmoid to generate a predicted output ($y$).\n",
    "Loss Calculation: The model calculates the Loss, which represents the difference between the predicted output and the actual target value.\n",
    "Optimization (Gradient Descent): The network uses Gradient Descent to reduce the loss.Parameter Modification: Learning occurs by systematically modifying the weights and biases based on the loss calculated.\n",
    "Iteration: This cycle repeats until the predicted output is as close as possible to the actual value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b1512",
   "metadata": {},
   "source": [
    "## 10. What is an optimizer and why is it needed?\n",
    "\n",
    "Architecture Definition: The process begins by defining the structure of the network, including the input, hidden (HL), and output layers.Initialization: The weights ($w$) and biases ($b$) are initially assigned random values.Prediction (Forward Pass): The network takes inputs ($x$) and processes them through neurons using mathematical functions, such as $y \\approx f(x_1, x_2)$, and activation functions like Sigmoid to generate a predicted output ($y$).\n",
    "Loss Calculation: The model calculates the Loss, which represents the difference between the predicted output and the actual target value.Optimization (Gradient Descent): The network uses Gradient Descent to reduce the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc5e717",
   "metadata": {},
   "source": [
    "## 11. Describe common optimizers.\n",
    "\n",
    "Gradient Descent: This is the primary tool used to reduce the \"loss\" (error) of the model.Loss Reduction: The fundamental goal of any optimizer in this framework is to reduce the loss by modifying the weights ($w$) and biases ($b$).\n",
    "Weight and Bias Modification: Optimizers work by iteratively adjusting parameters so that the predicted $y$ becomes as close as possible to the actual target.\n",
    "Random Initialization: The optimization process typically begins after weights and biases are randomly initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9428f5",
   "metadata": {},
   "source": [
    "## 12. Explain forward and backward propagation.\n",
    "Forward PropagationForward propagation is the process where the network takes input data and passes it through layers to generate a prediction.+1Input and Initialization: The process begins with defined inputs ($x$) and weights and biases that are randomly initialized.Mathematical Transformation: Each neuron calculates a weighted sum of inputs and adds a bias, such as $w_1x_1 + w_2x_2 + b_1$.\n",
    "Activation: This sum is passed through an activation function, like Sigmoid, to produce an output.Generating Prediction: The data moves from the input layer through the Hidden Layers (HL) to the output layer to produce the predicted value $y$.+2Backward PropagationBackward propagation is the learning phase where the network adjusts its internal parameters to improve accuracy.\n",
    "Loss Calculation: After the forward pass, the model calculates the Loss, which is the difference between the predicted value and the actual target.Gradient Descent: The network uses Gradient Descent to determine how to reduce this loss.Parameter Modification: The weights ($w$) and biases ($b$) are modified based on the error.Goal: This process repeats to make the prediction ($y$) as close as possible to the actual value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bdfe82",
   "metadata": {},
   "source": [
    "## 13. What is weight initialization?\n",
    "Weight initialization is a fundamental step in setting up the architecture of a neural network.Definition: It is the process where weights ($w$) and biases ($b$) are assigned their first values before the learning process begins.+1Method: In the standard deep learning workflow, these parameters are randomly initialized.Purpose: These initial values serve as the starting point for the model to begin its first \"Forward Pass\" to generate a prediction.Refinement: Once initialized, these values are not permanent; they are systematically modified during training through Gradient Descent to reduce the loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6085f39",
   "metadata": {},
   "source": [
    "## 14. What is the vanishing gradient problem?\n",
    "However, the notes do provide the foundational concepts related to this issue, which you can use to understand where the problem occurs in a neural network:\n",
    "Gradient Descent: The notes identify Gradient Descent as the process used to reduce loss by modifying weights and biases.\n",
    "Sigmoid Activation: The notes show the use of the Sigmoid activation function in calculations.\n",
    "Deep Architectures: The notes mention that Deep Learning involves multiple hidden layers to process data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b1a7c6",
   "metadata": {},
   "source": [
    "## 15. What is the exploding gradient problem?\n",
    "\n",
    "Gradient Descent: The notes identify Gradient Descent as the method used to reduce \"Loss\" by modifying weights ($w$) and biases ($b$).Parametric Modification: Learning involves systematically updating parameters to make the predicted output ($y$) as close as possible to the actual target.+1Mathematical Structure: Calculations involve slopes, intercepts, and curvatures within a multi-layered architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4770f6",
   "metadata": {},
   "source": [
    "## Practical 1: Simple Perceptron for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,0,0,1])\n",
    "\n",
    "weights = np.random.rand(2)\n",
    "bias = 0\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i in range(len(X)):\n",
    "        z = np.dot(X[i], weights) + bias\n",
    "        y_pred = 1 if z > 0 else 0\n",
    "        error = y[i] - y_pred\n",
    "        weights += lr * error * X[i]\n",
    "        bias += lr * error\n",
    "\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480ab29",
   "metadata": {},
   "source": [
    "## Practical 2: Neural Network with One Hidden Layer (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(8, activation='relu', input_shape=(2,)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8b499",
   "metadata": {},
   "source": [
    "## Practical 3: Xavier Initialization in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74411ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "Dense(10, activation='relu', kernel_initializer=GlorotUniform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874693e0",
   "metadata": {},
   "source": [
    "## Practical 4: Activation Functions, Dropout & Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a6f10",
   "metadata": {},
   "source": [
    "## Practical 5: Visualizing Training Accuracy & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9bb784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = {'accuracy':[0.6,0.75,0.85], 'loss':[0.9,0.5,0.3]}\n",
    "\n",
    "plt.plot(history['accuracy'], label='Accuracy')\n",
    "plt.plot(history['loss'], label='Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
